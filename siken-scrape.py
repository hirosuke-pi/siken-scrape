import pyperclip
import requests
from bs4 import BeautifulSoup
from typing import TypedDict

Answer = TypedDict('ç­”ãˆæƒ…å ±', {'answer': str, 'text': str, 'link': str, 'recommend': bool})


def main():
    print('ðŸ“„ âœï¸  è©¦é¨“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v1.1')
    print('ðŸŒŸ > ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ãƒ¯ãƒ¼ãƒ‰ãŒæ›¸ãè¾¼ã¾ã‚Œã‚‹ã¨ã€è‡ªå‹•çš„ã«æ¤œç´¢ã—ã¾ã™ã€‚\n')
    while True:
        try:
            #word = input('[Word]: ') # å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã«ã—ãŸã„å ´åˆã¯ã“ã“ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤
            word = pyperclip.waitForNewPaste()
            print('ðŸ”Ž > ' + word + '\n')

            ans_info = get_answer_withword(word)
            rank = 1
            for ans in ans_info:
                print('    (' + str(rank) + ') ' + ('[å®Œå…¨ä¸€è‡´âœ¨] ' if ans['recommend'] else '') + ans['link'])
                print('    (' + str(rank) + ') ' + ans['answer'] + ': ' + ans['text'] + '\n')
                rank += 1
        except Exception as e:
            print('    âŒ > ' + str(e))
        except KeyboardInterrupt:
            break
        print('')


def get_googlesearch_rank_withlink(search_word: str, page: int = 10) -> list[str]:
    url = f'https://www.google.co.jp/search?hl=ja&num={page}&q={search_word}'
    request = requests.get(url)

    soup = BeautifulSoup(request.text, "html.parser")
    search_site_list = soup.select('div.kCrYT > a')

    ranks_link = []
    for rank, site in zip(range(1, page + 1), search_site_list):
        site_url = site['href'].replace('/url?q=', '')
        ranks_link.append(site_url)

    return ranks_link


def parse_siken_webpage(request_text: str, word: str, link: str) -> Answer:
    ANS_SELECT = {'ã‚¢': 'a', 'ã‚¤': 'i', 'ã‚¦': 'u', 'ã‚¨': 'e', 'ã‚ª': 'o'}
    ANS_SELECT_NUM = {'ã‚¢': 0, 'ã‚¤': 1, 'ã‚¦': 2, 'ã‚¨': 3, 'ã‚ª': 4}
    word = word.replace('ã€', 'ï¼Œ')

    try:
        soup = BeautifulSoup(request_text, "html.parser")
        ans_el = soup.select('#answerChar')
        ans = ''
        ansText = ''

        if len(ans_el) > 0:
            ans = ans_el[0].text
            ansText = soup.select('#select_' + ANS_SELECT[ans])[0].text
        else:
            ans = soup.select('#true')[0].text
            ansText = soup.select('.selectList')[
                0].contents[ANS_SELECT_NUM[ans]].text

        return {'answer': ans, 'text': ansText, 'link': link, 'recommend': word in soup.select('main')[0].text}
    except:
        return None


def get_answer_withword(word: str) -> list[Answer]:
    ranks_link = get_googlesearch_rank_withlink(word)
    if len(ranks_link) <= 0:
        raise Exception('Googleæ¤œç´¢ã«ãƒ’ãƒƒãƒˆã—ã¾ã›ã‚“ã§ã—ãŸã€‚')

    ans_links = []
    for link in ranks_link:
        if '-siken.com' in link:
            ans_links.append(link.split('&sa=')[0])

    if len(ans_links) <= 0:
        raise Exception('ãƒ©ãƒ³ã‚­ãƒ³ã‚°å†…ã«è©¦é¨“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')

    ans_info = []
    for ans_link in ans_links:
        request = requests.get(ans_link)
        ans = parse_siken_webpage(request.text, word, ans_link)
        if ans is None:
            continue
        ans_info.append(ans)

    if len(ans_info) <= 0:
        raise Exception('è©¦é¨“æƒ…å ±å†…ã§ç­”ãˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')

    ans_info_recommends = []
    for ans in ans_info:
        if ans['recommend']:
            ans_info_recommends.append(ans)

    if len(ans_info_recommends) <= 0:
        ans_info_recommends = ans_info

    return ans_info_recommends


if __name__ == "__main__":
    main()
